model:
  vocab_size: 512 # Phải khớp với num_embeddings của quantizer
  d_model: 256
  nhead: 4
  num_encoder_layers: 4
  num_decoder_layers: 4
  dim_feedforward: 1024
  # Special tokens
  sos_idx: 510
  eos_idx: 511
  pad_idx: 0

training:
  epochs: 50
  batch_size: 32
  learning_rate: 0.0005
  output_dir: "./checkpoints/translator"